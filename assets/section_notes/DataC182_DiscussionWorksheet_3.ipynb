{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_qKFEzc3ADH"
      },
      "source": [
        "# Fully-Connected Neural Nets\n",
        "\n",
        "In this notebook we will implement fully-connected networks using a modular approach. For each layer we will implement a `forward` and a `backward` function. The `forward` function will receive inputs, weights, and other parameters and will return both an output and a `cache` object storing data needed for the backward pass, like this:\n",
        "\n",
        "```python\n",
        "def layer_forward(x, w):\n",
        "  \"\"\" Receive inputs x and weights w \"\"\"\n",
        "  # Do some computations ...\n",
        "  z = # ... some intermediate value\n",
        "  # Do some more computations ...\n",
        "  out = # the output\n",
        "   \n",
        "  cache = (x, w, z, out) # Values we need to compute gradients\n",
        "  \n",
        "  return out, cache\n",
        "```\n",
        "\n",
        "The backward pass will receive upstream derivatives and the `cache` object, and will return gradients with respect to the inputs and weights, like this:\n",
        "\n",
        "```python\n",
        "def layer_backward(dout, cache):\n",
        "  \"\"\"\n",
        "  Receive derivative of loss with respect to outputs and cache,\n",
        "  and compute derivative with respect to inputs.\n",
        "  \"\"\"\n",
        "  # Unpack cache values\n",
        "  x, w, z, out = cache\n",
        "  \n",
        "  # Use values in cache to compute derivatives\n",
        "  dx = # Derivative of loss with respect to x\n",
        "  dw = # Derivative of loss with respect to w\n",
        "  \n",
        "  return dx, dw\n",
        "```\n",
        "\n",
        "After implementing a bunch of layers this way, we will be able to easily combine them to build classifiers with different architectures.\n",
        "\n",
        "In addition to implementing fully-connected networks of arbitrary depth, we will also explore different update rules for optimization, and introduce Dropout as a regularizer and Batch Normalization as a tool to more efficiently optimize deep networks.\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ramimrad/datac182_discussion5_student.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATKLMQNG3Fvi",
        "outputId": "c26ce30e-3528-4736-882a-95ef88dfec2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'datac182_discussion5_student'...\n",
            "remote: Enumerating objects: 71, done.\u001b[K\n",
            "remote: Counting objects: 100% (71/71), done.\u001b[K\n",
            "remote: Compressing objects: 100% (63/63), done.\u001b[K\n",
            "remote: Total 71 (delta 20), reused 37 (delta 4), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (71/71), 1.08 MiB | 5.29 MiB/s, done.\n",
            "Resolving deltas: 100% (20/20), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/datac182_discussion5_student/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zD9-YQge8Va5",
        "outputId": "7858c500-1e16-480e-fb8b-75a946028d97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/datac182_discussion5_student\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tP0E64K3ADJ",
        "outputId": "4b14bc8c-6360-4fb8-fc52-d494c1b4c508"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run the following from the deeplearning directory and try again:\n",
            "python setup.py build_ext --inplace\n",
            "You may also need to restart your iPython kernel\n"
          ]
        }
      ],
      "source": [
        "# As usual, a bit of setup\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from deeplearning.classifiers.fc_net import *\n",
        "from deeplearning.data_utils import get_CIFAR10_data\n",
        "from deeplearning.gradient_check import eval_numerical_gradient, eval_numerical_gradient_array\n",
        "from deeplearning.solver import Solver\n",
        "import random\n",
        "import torch\n",
        "seed = 7\n",
        "torch.manual_seed(seed)\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "# for auto-reloading external modules\n",
        "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "def rel_error(x, y):\n",
        "    \"\"\" returns relative error \"\"\"\n",
        "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62lee0DF3ADL"
      },
      "source": [
        "# Affine layer: forward\n",
        "Open the file `deeplearning/layers.py` and implement the `affine_forward` function.\n",
        "\n",
        "Once you are done you can test your implementaion by running the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zVhSkbb3ADM"
      },
      "outputs": [],
      "source": [
        "# Test the affine_forward function\n",
        "\n",
        "num_inputs = 2\n",
        "input_shape = (4, 5, 6)\n",
        "output_dim = 3\n",
        "\n",
        "input_size = num_inputs * np.prod(input_shape)\n",
        "weight_size = output_dim * np.prod(input_shape)\n",
        "\n",
        "x = np.linspace(-0.1, 0.5, num=input_size).reshape(num_inputs, *input_shape)\n",
        "w = np.linspace(-0.2, 0.3, num=weight_size).reshape(np.prod(input_shape), output_dim)\n",
        "b = np.linspace(-0.3, 0.1, num=output_dim)\n",
        "\n",
        "out, _ = affine_forward(x, w, b)\n",
        "correct_out = np.array([[ 1.49834967,  1.70660132,  1.91485297],\n",
        "                        [ 3.25553199,  3.5141327,   3.77273342]])\n",
        "\n",
        "# Compare your output with ours. The error should be around 1e-9.\n",
        "print('Testing affine_forward function:')\n",
        "print('difference: ', rel_error(out, correct_out))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5Rm2Qit3ADM"
      },
      "source": [
        "# Affine layer: backward\n",
        "Now implement the `affine_backward` function and test your implementation using numeric gradient checking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OGk3Craz3ADN"
      },
      "outputs": [],
      "source": [
        "# Test the affine_backward function\n",
        "\n",
        "x = np.random.randn(10, 2, 3)\n",
        "w = np.random.randn(6, 5)\n",
        "b = np.random.randn(5)\n",
        "dout = np.random.randn(10, 5)\n",
        "\n",
        "dx_num = eval_numerical_gradient_array(lambda x: affine_forward(x, w, b)[0], x, dout)\n",
        "dw_num = eval_numerical_gradient_array(lambda w: affine_forward(x, w, b)[0], w, dout)\n",
        "db_num = eval_numerical_gradient_array(lambda b: affine_forward(x, w, b)[0], b, dout)\n",
        "\n",
        "_, cache = affine_forward(x, w, b)\n",
        "dx, dw, db = affine_backward(dout, cache)\n",
        "# The error should be around 1e-10\n",
        "print('Testing affine_backward function:')\n",
        "print('dx error: ', rel_error(dx_num, dx))\n",
        "print('dw error: ', rel_error(dw_num, dw))\n",
        "print('db error: ', rel_error(db_num, db))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KN7KOB6K3ADN"
      },
      "source": [
        "# ReLU layer: forward\n",
        "Implement the forward pass for the ReLU activation function in the `relu_forward` function and test your implementation using the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SN8Ho8p-3ADO"
      },
      "outputs": [],
      "source": [
        "# Test the relu_forward function\n",
        "\n",
        "x = np.linspace(-0.5, 0.5, num=12).reshape(3, 4)\n",
        "\n",
        "out, _ = relu_forward(x)\n",
        "correct_out = np.array([[ 0.,          0.,          0.,          0.,        ],\n",
        "                        [ 0.,          0.,          0.04545455,  0.13636364,],\n",
        "                        [ 0.22727273,  0.31818182,  0.40909091,  0.5,       ]])\n",
        "\n",
        "# Compare your output with ours. The error should be around 1e-8\n",
        "print('Testing relu_forward function:')\n",
        "print('difference: ', rel_error(out, correct_out))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTK6kqjd3ADO"
      },
      "source": [
        "# ReLU layer: backward\n",
        "Now implement the backward pass for the ReLU activation function in the `relu_backward` function and test your implementation using numeric gradient checking. Note that the ReLU activation is not differentiable at 0, but typically we don't worry about this and simply assign either 0 or 1 as the derivative by convention."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSolaHYC3ADO"
      },
      "outputs": [],
      "source": [
        "x = np.random.randn(10, 10)\n",
        "dout = np.random.randn(*x.shape)\n",
        "\n",
        "dx_num = eval_numerical_gradient_array(lambda x: relu_forward(x)[0], x, dout)\n",
        "\n",
        "_, cache = relu_forward(x)\n",
        "dx = relu_backward(dout, cache)\n",
        "\n",
        "# The error should be around 1e-12\n",
        "print('Testing relu_backward function:')\n",
        "print('dx error: ', rel_error(dx_num, dx))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSg_wPzh3ADO"
      },
      "source": [
        "# \"Sandwich\" layers\n",
        "There are some common patterns of layers that are frequently used in neural nets. For example, affine layers are frequently followed by a ReLU nonlinearity. To make these common patterns easy, we define several convenience layers in the file `deeplearning/layer_utils.py`.\n",
        "\n",
        "For now take a look at the `affine_relu_forward` and `affine_relu_backward` functions, and run the following to numerically gradient check the backward pass:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tq6nGors3ADP"
      },
      "outputs": [],
      "source": [
        "from deeplearning.layer_utils import affine_relu_forward, affine_relu_backward\n",
        "\n",
        "x = np.random.randn(2, 3, 4)\n",
        "w = np.random.randn(12, 10)\n",
        "b = np.random.randn(10)\n",
        "dout = np.random.randn(2, 10)\n",
        "\n",
        "out, cache = affine_relu_forward(x, w, b)\n",
        "dx, dw, db = affine_relu_backward(dout, cache)\n",
        "\n",
        "dx_num = eval_numerical_gradient_array(lambda x: affine_relu_forward(x, w, b)[0], x, dout)\n",
        "dw_num = eval_numerical_gradient_array(lambda w: affine_relu_forward(x, w, b)[0], w, dout)\n",
        "db_num = eval_numerical_gradient_array(lambda b: affine_relu_forward(x, w, b)[0], b, dout)\n",
        "\n",
        "print('Testing affine_relu_forward:')\n",
        "print('dx error: ', rel_error(dx_num, dx))\n",
        "print('dw error: ', rel_error(dw_num, dw))\n",
        "print('db error: ', rel_error(db_num, db))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Batch Normalization\n",
        "One way to make deep networks easier to train is to use more sophisticated optimization procedures such as SGD+momentum, RMSProp, or Adam. Another strategy is to change the architecture of the network to make it easier to train. One idea along these lines is batch normalization which was proposed by [1].\n",
        "\n",
        "The idea is relatively straightforward. Machine learning methods tend to work better when their input data consists of uncorrelated features with zero mean and unit variance. When training a neural network, we can preprocess the data before feeding it to the network to explicitly decorrelate its features; this will ensure that the first layer of the network sees data that follows a nice distribution. However even if we preprocess the input data, the activations at deeper layers of the network will likely no longer be decorrelated and will no longer have zero mean or unit variance since they are output from earlier layers in the network. Even worse, during the training process the distribution of features at each layer of the network will shift as the weights of each layer are updated.\n",
        "\n",
        "The authors of [1] hypothesize that the shifting distribution of features inside deep neural networks may make training deep networks more difficult. To overcome this problem, [1] proposes to insert batch normalization layers into the network. At training time, a batch normalization layer uses a minibatch of data to estimate the mean and standard deviation of each feature. These estimated means and standard deviations are then used to center and normalize the features of the minibatch. A running average of these means and standard deviations is kept during training, and at test time these running averages are used to center and normalize features.\n",
        "\n",
        "It is possible that this normalization strategy could reduce the representational power of the network, since it may sometimes be optimal for certain layers to have features that are not zero-mean or unit variance. To this end, the batch normalization layer includes learnable shift and scale parameters for each feature dimension.\n",
        "\n",
        "[1] Sergey Ioffe and Christian Szegedy, \"Batch Normalization: Accelerating Deep Network Training by Reducing\n",
        "Internal Covariate Shift\", ICML 2015."
      ],
      "metadata": {
        "id": "RQPynT16_BHE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batch normalization: Forward\n",
        "In the file `deeplearning/layers.py`, implement the batch normalization forward pass in the function `batchnorm_forward`. Once you have done so, run the following to test your implementation."
      ],
      "metadata": {
        "id": "TC25JcGH_EYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the training-time forward pass by checking means and variances\n",
        "# of features both before and after batch normalization\n",
        "\n",
        "# Simulate the forward pass for a two-layer network\n",
        "N, D1, D2, D3 = 200, 50, 60, 3\n",
        "X = np.random.randn(N, D1)\n",
        "W1 = np.random.randn(D1, D2)\n",
        "W2 = np.random.randn(D2, D3)\n",
        "a = np.maximum(0, X.dot(W1)).dot(W2)\n",
        "\n",
        "print ('Before batch normalization:')\n",
        "print ('  means: ', a.mean(axis=0))\n",
        "print ('  stds: ', a.std(axis=0))\n",
        "\n",
        "# Means should be close to zero and stds close to one\n",
        "print ('After batch normalization (gamma=1, beta=0)')\n",
        "a_norm, _ = batchnorm_forward(a, np.ones(D3), np.zeros(D3), {'mode': 'train'})\n",
        "print ('  mean: ', a_norm.mean(axis=0))\n",
        "print ('  std: ', a_norm.std(axis=0))\n",
        "\n",
        "# Now means should be close to beta and stds close to gamma\n",
        "gamma = np.asarray([1.0, 2.0, 3.0])\n",
        "beta = np.asarray([11.0, 12.0, 13.0])\n",
        "a_norm, _ = batchnorm_forward(a, gamma, beta, {'mode': 'train'})\n",
        "print ('After batch normalization (nontrivial gamma, beta)')\n",
        "print ('  means: ', a_norm.mean(axis=0))\n",
        "print ('  stds: ', a_norm.std(axis=0))"
      ],
      "metadata": {
        "id": "Pg4NwqSn_HU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the test-time forward pass by running the training-time\n",
        "# forward pass many times to warm up the running averages, and then\n",
        "# checking the means and variances of activations after a test-time\n",
        "# forward pass.\n",
        "N, D1, D2, D3 = 200, 50, 60, 3\n",
        "W1 = np.random.randn(D1, D2)\n",
        "W2 = np.random.randn(D2, D3)\n",
        "\n",
        "bn_param = {'mode': 'train'}\n",
        "gamma = np.ones(D3)\n",
        "beta = np.zeros(D3)\n",
        "for t in range(50):\n",
        "    X = np.random.randn(N, D1)\n",
        "    a = np.maximum(0, X.dot(W1)).dot(W2)\n",
        "    batchnorm_forward(a, gamma, beta, bn_param)\n",
        "bn_param['mode'] = 'test'\n",
        "X = np.random.randn(N, D1)\n",
        "a = np.maximum(0, X.dot(W1)).dot(W2)\n",
        "a_norm, _ = batchnorm_forward(a, gamma, beta, bn_param)\n",
        "\n",
        "# Means should be close to zero and stds close to one, but will be\n",
        "# noisier than training-time forward passes.\n",
        "print ('After batch normalization (test-time):')\n",
        "print ('  means: ', a_norm.mean(axis=0))\n",
        "print ('  stds: ', a_norm.std(axis=0))"
      ],
      "metadata": {
        "id": "WVtdf82B_LC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batch Normalization: backward\n",
        "Now implement the backward pass for batch normalization in the function `batchnorm_backward`.\n",
        "\n",
        "To derive the backward pass you should write out the computation graph for batch normalization and backprop through each of the intermediate nodes. Some intermediates may have multiple outgoing branches; make sure to sum gradients across these branches in the backward pass.\n",
        "\n",
        "Once you have finished, run the following to numerically check your backward pass."
      ],
      "metadata": {
        "id": "ApcvVTnq_Pl5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient check batchnorm backward pass\n",
        "\n",
        "N, D = 4, 5\n",
        "x = 5 * np.random.randn(N, D) + 12\n",
        "gamma = np.random.randn(D)\n",
        "beta = np.random.randn(D)\n",
        "dout = np.random.randn(N, D)\n",
        "\n",
        "bn_param = {'mode': 'train'}\n",
        "fx = lambda x: batchnorm_forward(x, gamma, beta, bn_param)[0]\n",
        "fg = lambda a: batchnorm_forward(x, gamma, beta, bn_param)[0]\n",
        "fb = lambda b: batchnorm_forward(x, gamma, beta, bn_param)[0]\n",
        "\n",
        "dx_num = eval_numerical_gradient_array(fx, x, dout)\n",
        "da_num = eval_numerical_gradient_array(fg, gamma, dout)\n",
        "db_num = eval_numerical_gradient_array(fb, beta, dout)\n",
        "\n",
        "_, cache = batchnorm_forward(x, gamma, beta, bn_param)\n",
        "dx, dgamma, dbeta = batchnorm_backward(dout, cache)\n",
        "print ('dx error: ', rel_error(dx_num, dx))\n",
        "print ('dgamma error: ', rel_error(da_num, dgamma))\n",
        "print ('dbeta error: ', rel_error(db_num, dbeta))"
      ],
      "metadata": {
        "id": "ksQmVxTv_Px2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batch Normalization: alternative backward\n",
        "In class we talked about two different implementations for the sigmoid backward pass. One strategy is to write out a computation graph composed of simple operations and backprop through all intermediate values. Another strategy is to work out the derivatives on paper. For the sigmoid function, it turns out that you can derive a very simple formula for the backward pass by simplifying gradients on paper.\n",
        "\n",
        "Surprisingly, it turns out that you can also derive a simple expression for the batch normalization backward pass if you work out derivatives on paper and simplify. After doing so, implement the simplified batch normalization backward pass in the function `batchnorm_backward_alt` and compare the two implementations by running the following. Your two implementations should compute nearly identical results, but the alternative implementation should be a bit faster.\n",
        "\n",
        "NOTE: You can still complete the rest of the assignment if you don't figure this part out, so don't worry too much if you can't get it."
      ],
      "metadata": {
        "id": "3k8wHVmw_RUe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N, D = 100, 500\n",
        "x = 5 * np.random.randn(N, D) + 12\n",
        "gamma = np.random.randn(D)\n",
        "beta = np.random.randn(D)\n",
        "dout = np.random.randn(N, D)\n",
        "\n",
        "bn_param = {'mode': 'train'}\n",
        "out, cache = batchnorm_forward(x, gamma, beta, bn_param)\n",
        "\n",
        "t1 = time.time()\n",
        "# repeat backwards passes multiple times for stability\n",
        "for r in range(1000):\n",
        "    dx1, dgamma1, dbeta1 = batchnorm_backward(dout, cache)\n",
        "t2 = time.time()\n",
        "for r in range(1000):\n",
        "    dx2, dgamma2, dbeta2 = batchnorm_backward_alt(dout, cache)\n",
        "t3 = time.time()\n",
        "\n",
        "print ('dx difference: ', rel_error(dx1, dx2))\n",
        "print ('dgamma difference: ', rel_error(dgamma1, dgamma2))\n",
        "print ('dbeta difference: ', rel_error(dbeta1, dbeta2))\n",
        "print ('speedup: %.2fx' % ((t2 - t1) / (t3 - t2)))"
      ],
      "metadata": {
        "id": "sNzyB2vo_S8p"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}